"use strict";(globalThis.webpackChunkbook_site=globalThis.webpackChunkbook_site||[]).push([[158],{4866(e,t,n){n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>i,metadata:()=>o,toc:()=>h});const o=JSON.parse('{"id":"chapter-4","title":"Chapter 4: Building the RAG Chatbot","description":"In the previous chapter, we meticulously crafted a specification for our GeminiModel client. Now, it\'s time to put that blueprint into action and build the core of our intelligent application: a Retrieval-Augmented Generation (RAG) chatbot. This chapter will guide you through the architecture and implementation of a chatbot that can answer questions based on a specific set of documents.","source":"@site/docs/chapter-4.md","sourceDirName":".","slug":"/chapter-4","permalink":"/ai-specbook/docs/chapter-4","draft":false,"unlisted":false,"editUrl":"https://github.com/AnsharaAmir/ai-specbook/tree/main/docs/chapter-4.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: Writing Your First Spec with Spec-Kit Plus","permalink":"/ai-specbook/docs/chapter-3"},"next":{"title":"Chapter 5: User Authentication and Personalization","permalink":"/ai-specbook/docs/chapter-5"}}');var r=n(4848),s=n(8453);const i={sidebar_position:4},a="Chapter 4: Building the RAG Chatbot",c={},h=[{value:"The RAG Architecture",id:"the-rag-architecture",level:2},{value:"1. FastAPI: The API Backbone",id:"1-fastapi-the-api-backbone",level:3},{value:"2. Qdrant: The Knowledge Base",id:"2-qdrant-the-knowledge-base",level:3},{value:"3. Neon: The Conversation Memory",id:"3-neon-the-conversation-memory",level:3},{value:"Selected-Text-Only Answer Mode",id:"selected-text-only-answer-mode",level:2},{value:"What&#39;s Next?",id:"whats-next",level:2}];function l(e){const t={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.header,{children:(0,r.jsx)(t.h1,{id:"chapter-4-building-the-rag-chatbot",children:"Chapter 4: Building the RAG Chatbot"})}),"\n",(0,r.jsxs)(t.p,{children:["In the previous chapter, we meticulously crafted a specification for our ",(0,r.jsx)(t.code,{children:"GeminiModel"})," client. Now, it's time to put that blueprint into action and build the core of our intelligent application: a Retrieval-Augmented Generation (RAG) chatbot. This chapter will guide you through the architecture and implementation of a chatbot that can answer questions based on a specific set of documents."]}),"\n",(0,r.jsx)(t.h2,{id:"the-rag-architecture",children:"The RAG Architecture"}),"\n",(0,r.jsxs)(t.p,{children:["A RAG chatbot is a powerful combination of two concepts: information retrieval and text generation. Instead of relying solely on the vast, general knowledge of a large language model (LLM), a RAG system first ",(0,r.jsx)(t.em,{children:"retrieves"})," relevant information from a private knowledge base and then uses that information to ",(0,r.jsx)(t.em,{children:"generate"})," a precise, context-aware answer."]}),"\n",(0,r.jsx)(t.p,{children:"Our architecture will be composed of three key components:"}),"\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"FastAPI"}),": Our web backend, providing the API endpoints for our chatbot."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Qdrant"}),": Our vector database, which will store our documents and allow us to find relevant passages."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Neon"}),": Our serverless Postgres database, used for storing conversation history."]}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"Here's a visual overview of how these components interact:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-mermaid",children:"graph TD\n    A[User] --\x3e|Sends question| B(FastAPI Backend)\n    B --\x3e|1. Embed question| C(Gemini API)\n    B --\x3e|2. Search for similar vectors| D(Qdrant Vector DB)\n    D --\x3e|Returns relevant text chunks| B\n    B --\x3e|3. Construct prompt with context| C\n    C --\x3e|Generates answer| B\n    B --\x3e|4. Store conversation| E(Neon Postgres DB)\n    B --\x3e|Returns answer| A\n"})}),"\n",(0,r.jsx)(t.h3,{id:"1-fastapi-the-api-backbone",children:"1. FastAPI: The API Backbone"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.a,{href:"https://fastapi.tiangolo.com/",children:"FastAPI"})," is a modern, high-performance web framework for building APIs with Python. It's incredibly fast, easy to use, and comes with automatic interactive documentation, which is perfect for our project. We'll use it to create an endpoint that accepts a user's question and returns a generated answer."]}),"\n",(0,r.jsx)(t.p,{children:"Here's a simplified example of what our main endpoint might look like:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'# main.py (FastAPI application)\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom my_rag_logic import get_rag_answer\n\napp = FastAPI()\n\nclass Query(BaseModel):\n    question: str\n    conversation_id: str | None = None\n\n@app.post("/chat")\nasync def chat(query: Query):\n    """\n    Receives a question, retrieves relevant context,\n    and returns a generated answer.\n    """\n    answer, new_conversation_id = await get_rag_answer(\n        query.question, query.conversation_id\n    )\n    return {"answer": answer, "conversation_id": new_conversation_id}\n'})}),"\n",(0,r.jsx)(t.h3,{id:"2-qdrant-the-knowledge-base",children:"2. Qdrant: The Knowledge Base"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.a,{href:"https://qdrant.tech/",children:"Qdrant"}),' is a vector database. It\'s designed to store high-dimensional vectors (in our case, text embeddings) and find the closest matches for a given query vector. This is the "retrieval" part of RAG.']}),"\n",(0,r.jsx)(t.p,{children:"Our process will be:"}),"\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Chunking:"})," Break our source documents (e.g., the chapters of this book) into smaller, manageable chunks."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Embedding:"})," Use a language model (like Gemini) to convert each text chunk into a numerical vector (an embedding)."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Indexing:"})," Store these vectors in a Qdrant collection."]}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"When a user asks a question, we'll embed their question into a vector and use Qdrant to find the text chunks with the most similar vectors. These chunks become the context for our answer."}),"\n",(0,r.jsx)(t.h3,{id:"3-neon-the-conversation-memory",children:"3. Neon: The Conversation Memory"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.a,{href:"https://neon.tech/",children:"Neon"}),' is a serverless, developer-friendly Postgres database. We\'ll use it to store the history of our conversations. Why is this important? It allows the chatbot to have "memory" and understand the context of a follow-up question.']}),"\n",(0,r.jsx)(t.p,{children:"For each question and answer, we'll store:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:["A unique ",(0,r.jsx)(t.code,{children:"conversation_id"}),"."]}),"\n",(0,r.jsx)(t.li,{children:"The user's question."}),"\n",(0,r.jsx)(t.li,{children:"The model's answer."}),"\n",(0,r.jsx)(t.li,{children:"A timestamp."}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"This allows us to rebuild the conversation history and provide it to the Gemini model, leading to more natural and coherent interactions."}),"\n",(0,r.jsx)(t.h2,{id:"selected-text-only-answer-mode",children:"Selected-Text-Only Answer Mode"}),"\n",(0,r.jsxs)(t.p,{children:["One of the challenges with LLMs is that they can sometimes \"hallucinate\" or provide information that isn't from the source material. To combat this, we'll implement a ",(0,r.jsx)(t.strong,{children:'"selected-text-only answer mode."'})]}),"\n",(0,r.jsxs)(t.p,{children:["This is a special instruction we give to the model in our prompt. We essentially tell it: ",(0,r.jsx)(t.strong,{children:'"Only use the information provided in the retrieved text to answer the question. If the answer is not in the text, say so."'})]}),"\n",(0,r.jsxs)(t.p,{children:["Here's how we can modify our prompt to the ",(0,r.jsx)(t.code,{children:"GeminiModel"})," to enable this mode:"]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'# Part of our get_rag_answer logic\nretrieved_chunks = qdrant_client.search(...) # Search Qdrant\n\ncontext = "\\n".join(chunk.text for chunk in retrieved_chunks)\n\nprompt = f"""\nYou are a helpful assistant. A user has asked a question.\nUsing ONLY the following text, please answer the user\'s question.\nDo not use any of your own knowledge. If the answer is not present in the text,\nrespond with "I could not find an answer in the provided text."\n\n---\nTEXT:\n{context}\n---\n\nQUESTION:\n{user_question}\n"""\n\nanswer = gemini_model.call(prompt=prompt)\n'})}),"\n",(0,r.jsx)(t.p,{children:"This technique forces the model to act as a pure question-answering system based on the context we provide, making our RAG chatbot more reliable and trustworthy."}),"\n",(0,r.jsx)(t.h2,{id:"whats-next",children:"What's Next?"}),"\n",(0,r.jsx)(t.p,{children:"We've now designed the architecture for our RAG chatbot. We understand the role of each component and have a strategy for ensuring our answers are grounded in our source documents."}),"\n",(0,r.jsx)(t.p,{children:"In the next chapter, we'll dive into the implementation details. We'll set up our project, write the code to chunk and embed our documents, and build the FastAPI application that ties everything together."})]})}function d(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},8453(e,t,n){n.d(t,{R:()=>i,x:()=>a});var o=n(6540);const r={},s=o.createContext(r);function i(e){const t=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),o.createElement(s.Provider,{value:t},e.children)}}}]);